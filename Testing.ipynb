{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv('LP-IV-datasets/CIFR(Ass2&3)/train_data.csv')\n",
    "test_data = pd.read_csv('LP-IV-datasets/CIFR(Ass2&3)/test_data.csv')\n",
    "\n",
    "train_images = train_data.drop('label', axis=1).values\n",
    "train_labels = train_data['label'].values\n",
    "\n",
    "test_images = test_data.drop('label', axis=1).values\n",
    "test_labels = test_data['label'].values\n",
    "\n",
    "train_images = train_images.reshape((train_images.shape[0], 32, 32, 3))\n",
    "test_images = test_images.reshape((test_images.shape[0], 32, 32, 3))\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels)\n",
    "\n",
    "# Define the network architecture using Keras Sequential API\n",
    "model = models.Sequential()\n",
    "model.add(layers.Flatten(input_shape=(32, 32, 3)))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the training data\n",
    "history = model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f\"\\nTest accuracy: {test_acc}\")\n",
    "\n",
    "# Plot training loss and accuracy\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot training loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#############################################\n",
    "#############################################\n",
    "#############################################\n",
    "#############################################\n",
    "#############################################\n",
    "#############################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('LP-IV-datasets/CIFR(Ass2&3)/train_data.csv')\n",
    "test_data = pd.read_csv('LP-IV-datasets/CIFR(Ass2&3)/test_data.csv')\n",
    "\n",
    "train_images = train_data.drop('label', axis=1).values\n",
    "train_labels = train_data['label'].values\n",
    "\n",
    "test_images = test_data.drop('label', axis=1).values\n",
    "test_labels = test_data['label'].values\n",
    "\n",
    "train_images = train_images.reshape((train_images.shape[0], 32, 32, 3)).astype('float') / 255.0\n",
    "test_images = test_images.reshape((test_images.shape[0], 32, 32, 3)).astype('float') / 255.0\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "# train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network architecture using Keras Sequential API\n",
    "model = models.Sequential()\n",
    "model.add(layers.Flatten(input_shape=(32, 32, 3)))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile( loss='sparse_categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using the training data\n",
    "history = model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f\"\\nTest accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss and accuracy\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot training loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN_cifar\n",
    "# Build the CNN model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "# CNN_mnist\n",
    "# Build a CNN model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "# ff_cifar\n",
    "model = models.Sequential()\n",
    "model.add(layers.Flatten(input_shape=(32, 32, 3)))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "# ff_mnist\n",
    "model = models.Sequential()\n",
    "model.add(layers.Flatten(input_shape=(28, 28, 1)))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBOW code\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Lambda\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import re\n",
    "\n",
    "data = \"\"\"\n",
    "The speed of transmission is an important point of difference between the two viruses. Influenza has a shorter median incubation period (the time from infection to appearance of symptoms) and a shorter serial interval (the time between successive cases) than COVID-19 virus. The serial interval for COVID-19 virus is estimated to be 5-6 days, while for influenza virus, the serial interval is 3 days. This means that influenza can spread faster than COVID-19. \n",
    "Further, transmission in the first 3-5 days of illness, or potentially pre-symptomatic transmission –transmission of the virus before the appearance of symptoms – is a major driver of transmission for influenza. In contrast, while we are learning that there are people who can shed COVID-19 virus 24-48 hours prior to symptom onset, at present, this does not appear to be a major driver of transmission. \n",
    "The reproductive number – the number of secondary infections generated from one infected individual – is understood to be between 2 and 2.5 for COVID-19 virus, higher than for influenza. However, estimates for both COVID-19 and influenza viruses are very context and time-specific, making direct comparisons more difficult.  \n",
    "\"\"\"\n",
    "\n",
    "sentences = data.split(\".\")\n",
    "\n",
    "clean_sentences = []\n",
    "for sentence in sentences:\n",
    "    if sentence == \"\":\n",
    "        continue\n",
    "    sentence = re.sub('[^A-Za-z0-9]+', ' ', sentence)\n",
    "    sentence = re.sub(r'(?:^| )\\w(?:$| )', ' ', sentence).strip()\n",
    "    sentence = sentence.lower()\n",
    "    clean_sentences.append(sentence)\n",
    "\n",
    "corpus = clean_sentences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "print(sequences)\n",
    "\n",
    "index_to_word_map = {}\n",
    "word_to_index_map = {}\n",
    "\n",
    "for index1, sequence in enumerate(sequences):\n",
    "    word_set = clean_sentences[index1].split()\n",
    "    for index2, value in enumerate(sequence):\n",
    "        index_to_word_map[value] = word_set[index2]\n",
    "        word_to_index_map[word_set[index2]] = value\n",
    "\n",
    "print(index_to_word_map)\n",
    "print(word_to_index_map)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_size = 10\n",
    "window_size = 2\n",
    "\n",
    "contexts = []\n",
    "targets = []\n",
    "\n",
    "for sequence in sequences:\n",
    "    for i in range(window_size, len(sequence) - window_size):\n",
    "        context = sequence[i - window_size:i] + sequence[i + 1:i + window_size + 1]\n",
    "        target = sequence[i]\n",
    "        contexts.append(context)\n",
    "        targets.append(target)\n",
    "\n",
    "X = np.array(contexts)\n",
    "Y = np.array(targets)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=2 * window_size))\n",
    "model.add(Lambda(lambda x: tf.reduce_mean(x, axis=1)))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(units=vocab_size, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, Y, epochs=200, verbose=1)\n",
    "\n",
    "# Get the word embeddings\n",
    "embeddings = model.get_weights()[0]\n",
    "\n",
    "# Perform PCA to reduce the dimensionality of the embeddings\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Visualize the embeddings\n",
    "plt.figure(figsize=(7, 7))\n",
    "for i, word in enumerate(tokenizer.word_index.keys()):\n",
    "    x, y = reduced_embeddings[i]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(word, xy=(x, y), xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right', va='bottom')\n",
    "plt.show()\n",
    "\n",
    "# Updated test model sentences\n",
    "test_sentences = [\n",
    "    \"the speed of transmission\",\n",
    "    \"shorter median incubation period\",\n",
    "    \"transmission in the first 3-5 days\",\n",
    "    \"reproductive number the number of secondary infections\"\n",
    "]\n",
    "\n",
    "for test_sentence in test_sentences:\n",
    "    test_words = test_sentence.split(\" \")\n",
    "    print(\"Words: \", test_words)\n",
    "    x_test = [word_to_index_map.get(word, 0) for word in test_words]  # Use 0 if the word is not in the vocabulary\n",
    "    x_test = np.array([x_test], dtype=np.int32)\n",
    "    print(\"Indices: \", x_test)\n",
    "    test_predictions = model.predict(x_test)\n",
    "    y_pred = np.argmax(test_predictions[0])\n",
    "    print(\"Predictions: \", test_words, \" => \", index_to_word_map.get(y_pred))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object Detection\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "# dataset_dir = \"LP-IV-datasets/Object_Detection(Ass6)/caltech-101-img\"\n",
    "dataset_dir = \"D:\\BE1\\DL_practical\\Aditya\\LP-IV-datasets\\Object Detection(Ass6)\\caltech-101-img\"\n",
    "\n",
    "dataset_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    ")\n",
    "\n",
    "# here batch_size is the number of images in each batch\n",
    "batch_size = 2000\n",
    "dataset_generator = dataset_datagen.flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=(64, 64),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "x_train, y_train =  dataset_generator[0]\n",
    "x_test, y_test = dataset_generator[1]\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(x_test))\n",
    "\n",
    "# Load VGG16 without top layers\n",
    "# weights_path = \"./LP-IV-datasets/Object_Detection(Ass6)/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "weights_path = \"D:\\BE1\\DL_practical\\Aditya\\LP-IV-datasets\\Object Detection(Ass6)\\vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "base_model = VGG16(weights=weights_path, include_top=False, input_shape=(64, 64, 3))\n",
    "\n",
    "for layer in base_model.layers:\n",
    "   layer.trainable = False\n",
    "\n",
    "base_model.summary()\n",
    "\n",
    "\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "predictions = Dense(102, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "base_model = VGG16(weights=weights_path, include_top=False, input_shape=(64, 64, 3))\n",
    "# freeze all layers first\n",
    "for layer in base_model.layers:\n",
    "   layer.trainable = False\n",
    "# unfreeze last 4 layers of base model\n",
    "for layer in base_model.layers[len(base_model.layers) - 2:]:\n",
    "   layer.trainable = True\n",
    "# fine-tuning hyper parameters\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "predictions = Dense(102, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# training fine tuned model\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "predicted_value = model.predict(x_test)\n",
    "\n",
    "labels = list(dataset_generator.class_indices.keys())\n",
    "\n",
    "\n",
    "n = 1001\n",
    "plt.imshow(x_test[n])\n",
    "print(\"Preditcted: \",labels[np.argmax(predicted_value[n])])\n",
    "print(\"Actual: \", labels[np.argmax(y_test[n])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#auto encoder\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ecg_data = pd.read_csv('LP-IV-datasets/ECGdataset(Ass4)/ecg_autoencoder_dataset.csv')\n",
    "\n",
    "scalar = StandardScaler()\n",
    "X = scalar.fit_transform(ecg_data.values)\n",
    "y = X\n",
    "\n",
    "X_train,X_test,_,_ = train_test_split(X,X,test_size=0.2,random_state=42)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "encoder = models.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(32,activation='relu'),\n",
    "        layers.Dense(16,activation='relu'),\n",
    "        layers.Dense(8,activation='relu'),\n",
    "    \n",
    "    ]\n",
    ")\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "decoder = models.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(8,)),\n",
    "        layers.Dense(16,activation='relu'),\n",
    "        layers.Dense(32,activation='relu'),\n",
    "        layers.Dense(input_dim,activation='linear'),\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "autoencoder = models.Sequential([\n",
    "    encoder,\n",
    "    decoder\n",
    "])\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "autoencoder.fit(X_train, X_train, epochs=100, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "y_pred = autoencoder.predict(X_test)\n",
    "mse = np.mean(np.power((y_pred - X_test),2),axis=1)\n",
    "\n",
    "\n",
    "\n",
    "threshold = np.percentile(mse,95)\n",
    "\n",
    "anamolies = mse > threshold\n",
    "\n",
    "num_anamolies = np.sum(anamolies)\n",
    "print(f\"Number of Anomalies: {num_anamolies}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(X_test[0], label='Original ECG')\n",
    "plt.plot(y_pred[0], label='Reconstructed ECG')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.title('Normal ECG')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
